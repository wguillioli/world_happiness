census <- read.csv("./data/_states/us_census_firms_2016.csv", stringsAsFactors = FALSE)
census <- census[census$employment_size == "01:  Total",] #Keep only row with total per area/state
census$establishments_pct <- census$establishments_num / 7757807 #% of establishments in US
census <- census[,c("area", "establishments_num", "establishments_pct")] #Keep only columns of interest
#Merge/Join data to have one master dataset
d <- merge(d, regions, by.x = "Name", by.y = "State", all.x = TRUE)
d <- merge(d, pop, by.x = "Name", by.y = "NAME", all.x = TRUE)
d <- merge(d, party, by.x = "Name", by.y = "State", all.x = TRUE)
d <- merge(d, gdp, by.x = "Name", by.y = "State", all.x = TRUE)
d <- merge(d, census, by.x = "Name", by.y = "area", all.x = TRUE)
#Keep only states with estimate data
d <- d[d$Status %in% c("Done", "Gaurav did"),]
summary(d)
cor(d$gdp_mm, d$estimate)
cor(d$gdp_mm, d$estimate)
str(d)
str(gdp)
gdp$gdp_mm <- as.integer(gdp$gdp_mm)
#Load CSV file from Excel with data collected for the states
d <- read.csv("states-work-tracker.csv", stringsAsFactors = FALSE)
str(d)
summary(d)
table(d$Status, useNA = "always") #36 states are done as expected
d <- d[,c(2:7)] #drop raw and unneeded cols
head(d)
#Load CSV with region and division for each state
regions <- read.csv("./data/_states/states_region_division.csv", stringsAsFactors = FALSE)
regions <- regions[,-c(2)] #drop state code
#Load CSV with population estimate by state
pop <- read.csv("./data/_states/nst-est2019-alldata.csv", stringsAsFactors = FALSE)
pop <- pop[pop$STATE != 0 & pop$STATE != 72,] #Excluse summary rows and Puerto Rico from file
pop <- pop[,c("NAME","POPESTIMATE2019")] #Keep only columns of interest (state name and 2019 population)
sum(pop$POPESTIMATE2019) #328.2M is estimated population
pop$popestimate2019pct <- pop$POPESTIMATE2019 / sum(pop$POPESTIMATE2019)
#Load CSV with political party winnining 2016 election
party <- read.csv("./data/_states/presidential_election.csv", stringsAsFactors = FALSE)
#Load CSV with GDP by state
gdp <- read.csv("./data/_states/gdp_by_state.csv", stringsAsFactors = FALSE)
gdp <- gdp[,-c(1)] #remove rank column
gdp$State <- trimws(gdp$State) #remove white space from name
gdp$gdp_pct <- gdp$gdp_pct / 100 #make it as actual pct
gdp$gdp_mm <- as.integer(gdp$gdp_mm)
#Load US Census data for # of businesses per state
census <- read.csv("./data/_states/us_census_firms_2016.csv", stringsAsFactors = FALSE)
census <- census[census$employment_size == "01:  Total",] #Keep only row with total per area/state
census$establishments_pct <- census$establishments_num / 7757807 #% of establishments in US
census <- census[,c("area", "establishments_num", "establishments_pct")] #Keep only columns of interest
#Merge/Join data to have one master dataset
d <- merge(d, regions, by.x = "Name", by.y = "State", all.x = TRUE)
d <- merge(d, pop, by.x = "Name", by.y = "NAME", all.x = TRUE)
d <- merge(d, party, by.x = "Name", by.y = "State", all.x = TRUE)
d <- merge(d, gdp, by.x = "Name", by.y = "State", all.x = TRUE)
d <- merge(d, census, by.x = "Name", by.y = "area", all.x = TRUE)
#Keep only states with estimate data
d <- d[d$Status %in% c("Done", "Gaurav did"),]
summary(d)
cor(d$gdp_mm, d$estimate)
cor(d$gdp_pct, d$estimate)
cor(d$gdp_mm, d$estimate)
cor(d$gdp_pct, d$estimate)
cor(d$gdp_mm, d$estimate)
cor(d$gdp_pct, d$estimate)
View(d)
#Load CSV file from Excel with data collected for the states
d <- read.csv("states-work-tracker.csv", stringsAsFactors = FALSE)
str(d)
summary(d)
table(d$Status, useNA = "always") #36 states are done as expected
d <- d[,c(2:7)] #drop raw and unneeded cols
head(d)
#Load CSV with region and division for each state
regions <- read.csv("./data/_states/states_region_division.csv", stringsAsFactors = FALSE)
regions <- regions[,-c(2)] #drop state code
#Load CSV with population estimate by state
pop <- read.csv("./data/_states/nst-est2019-alldata.csv", stringsAsFactors = FALSE)
pop <- pop[pop$STATE != 0 & pop$STATE != 72,] #Excluse summary rows and Puerto Rico from file
pop <- pop[,c("NAME","POPESTIMATE2019")] #Keep only columns of interest (state name and 2019 population)
sum(pop$POPESTIMATE2019) #328.2M is estimated population
pop$popestimate2019pct <- pop$POPESTIMATE2019 / sum(pop$POPESTIMATE2019)
#Load CSV with political party winnining 2016 election
party <- read.csv("./data/_states/presidential_election.csv", stringsAsFactors = FALSE)
#Load CSV with GDP by state
gdp <- read.csv("./data/_states/gdp_by_state.csv", stringsAsFactors = FALSE)
gdp <- gdp[,-c(1)] #remove rank column
gdp$State <- trimws(gdp$State) #remove white space from name
gdp$gdp_pct <- gdp$gdp_pct / 100 #make it as actual pct
#gdp$gdp_mm <- as.integer(gdp$gdp_mm)
#Load US Census data for # of businesses per state
census <- read.csv("./data/_states/us_census_firms_2016.csv", stringsAsFactors = FALSE)
census <- census[census$employment_size == "01:  Total",] #Keep only row with total per area/state
census$establishments_pct <- census$establishments_num / 7757807 #% of establishments in US
census <- census[,c("area", "establishments_num", "establishments_pct")] #Keep only columns of interest
#Merge/Join data to have one master dataset
d <- merge(d, regions, by.x = "Name", by.y = "State", all.x = TRUE)
d <- merge(d, pop, by.x = "Name", by.y = "NAME", all.x = TRUE)
d <- merge(d, party, by.x = "Name", by.y = "State", all.x = TRUE)
d <- merge(d, gdp, by.x = "Name", by.y = "State", all.x = TRUE)
d <- merge(d, census, by.x = "Name", by.y = "area", all.x = TRUE)
#Keep only states with estimate data
d <- d[d$Status %in% c("Done", "Gaurav did"),]
summary(d)
str(d)
cor(d$gdp_mm, d$estimate)
cor(d$gdp_pct, d$estimate)
ggplot(d, aes(x=estimate, y=gdp_pct)) +
geom_point() +
labs(title="\nEstimate of son to daughter ratio vs GDP % of US", x="\nEstimate of son to daughter ratio",y="\nGDP Percentage")
#ggtitle("Estimate of son to daughter ratio by US Region")
#geom_smooth(method=lm)
#ggplot(d, aes(x=estimate, y=popestimate2019pct, shape=Region, color=Region)) +
# geom_point()
#geom_smooth(method=lm)
ggplot(d, aes(x=estimate, y=gdp_pct)) +
geom_point() +
labs(title="\nEstimate of son to daughter ratio vs GDP % of US", x="\nEstimate of son to daughter ratio",y="\nGDP Percentage\n")
#ggtitle("Estimate of son to daughter ratio by US Region")
#geom_smooth(method=lm)
#ggplot(d, aes(x=estimate, y=popestimate2019pct, shape=Region, color=Region)) +
# geom_point()
#geom_smooth(method=lm)
View(d)
cor(d$establishments_num, d$estimate)
cor(d$establishments_pct, d$estimate)
cor(d$establishments_num, d$estimate)
cor(d$establishments_pct, d$estimate)
ggplot(d, aes(x=estimate, y=establishments_pct)) +
geom_point() +
labs(title="\nEstimate of son to daughter ratio vs # of Establishments", x="\nEstimate of son to daughter ratio",y="\nEstablishments\n")
#ggtitle("Estimate of son to daughter ratio by US Region")
#geom_smooth(method=lm)
#ggplot(d, aes(x=estimate, y=popestimate2019pct, shape=Region, color=Region)) +
# geom_point()
#geom_smooth(method=lm)
table(d$presidentialelection2016)
aggregate(estimate ~ presidentialelection2016, d, FUN = median)
ggplot(d, aes(x=presidentialelection2016, y=estimate)) +
geom_boxplot() +
xlab("\nPolitcal Party") +
ylab("\nEstimate of son to daughter ratio") +
ggtitle("Estimate of son to daughter ratio by mayority Political party (2016 elections)") +
coord_flip()
aggregate(estimate ~ presidentialelection2016, d, FUN = median)
d
View(d)
str(d)
d[,c("Name","estimate","son","daughter")]
d[,c("Name","estimate","son","daughter")]
library(tinytex)
install.packages("tinytex")
install.packages("latexpdf")
knitr::opts_chunk$set(echo = TRUE)
#Clean R environment
rm(list = ls())
#Set working directory
setwd("C:/Users/wguil/OneDrive/Documents/GitHub/sonny_side/")
#Load R packages
library(tidyverse)
library(tinytex)
library(latexpdf)
install.packages("miktex")
detach("package:tinytex", unload = TRUE)
tinytex:::is_tinytex() is TRUE
tinytex:::is_tinytex() == TRUE
render("sonny_side.Rmd", pdf_document())
knitr::render("sonny_side.Rmd", pdf_document())
rmakrdown::render("sonny_side.Rmd", pdf_document())
rmarkdown::render("sonny_side.Rmd", pdf_document())
rmarkdown::render("sonny_side.Rmd", pdf_document("mypdf.pdf"))
rmarkdown::render("sonny_side.Rmd", "pdf_document")
rmarkdown::render("C:/Users/wguil/OneDrive/Documents/GitHub/sonny_side/ms/sonny_side.Rmd", "pdf_document")
rmarkdown::render("C:/Users/wguil/OneDrive/Documents/GitHub/sonny_side/ms/sonny_side.Rmd", "pdf_document")
?knitr::knit
rmarkdown::render("C:/Users/wguil/OneDrive/Documents/GitHub/sonny_side/ms/sonny_side.Rmd", "pdf_document")
knitr::opts_chunk$set(echo = TRUE)
#Clean R environment
rm(list = ls())
#Load R packages
library(tidyverse)
library(tinytex)
library(latexpdf)
library(knitr)
#setwd - remove when we post in github
#setwd("C:/Users/wguil/OneDrive/Documents/GitHub/sonny_side/data/")
#Load CSV file from Excel with data collected for the states
d <- read.csv("../states-work-tracker.csv", stringsAsFactors = FALSE)
str(d)
summary(d)
table(d$Status, useNA = "always") #36 states are done as expected
d <- d[, c(2:7,11)] #drop raw and unneeded cols
head(d)
#Load CSV with region and division for each state
regions <- read.csv("../data/_states/states_region_division.csv", stringsAsFactors = FALSE)
regions <- regions[, -c(2)] #drop state code
#Load CSV with population estimate by state
pop <- read.csv("../data/_states/nst-est2019-alldata.csv", stringsAsFactors = FALSE)
pop <- pop[pop$STATE != 0 & pop$STATE != 72,] #Excluse summary rows and Puerto Rico from file
pop <- pop[,c("NAME","POPESTIMATE2019")] #Keep only columns of interest (state name and 2019 population)
sum(pop$POPESTIMATE2019) #328.2M is estimated population
pop$popestimate2019pct <- pop$POPESTIMATE2019 / sum(pop$POPESTIMATE2019)
#Load CSV with political party winnining 2016 election
party <- read.csv("../data/_states/presidential_election.csv", stringsAsFactors = FALSE)
#Load CSV with GDP by state
gdp <- read.csv("../data/_states/gdp_by_state.csv", stringsAsFactors = FALSE)
gdp <- gdp[,-c(1)] #remove rank column
gdp$State <- trimws(gdp$State) #remove white space from name
gdp$gdp_pct <- gdp$gdp_pct / 100 #make it as actual pct
#gdp$gdp_mm <- as.integer(gdp$gdp_mm)
#Load US Census data for # of businesses per state
census <- read.csv("../data/_states/us_census_firms_2016.csv", stringsAsFactors = FALSE)
census <- census[census$employment_size == "01:  Total",] #Keep only row with total per area/state
census$establishments_pct <- census$establishments_num / 7757807 #% of establishments in US
census <- census[, c("area", "establishments_num", "establishments_pct")] #Keep only columns of interest
#Merge/Join data to have one master dataset
d <- merge(d, regions, by.x = "Name", by.y = "State", all.x = TRUE)
d <- merge(d, pop, by.x = "Name", by.y = "NAME", all.x = TRUE)
d <- merge(d, party, by.x = "Name", by.y = "State", all.x = TRUE)
d <- merge(d, gdp, by.x = "Name", by.y = "State", all.x = TRUE)
d <- merge(d, census, by.x = "Name", by.y = "area", all.x = TRUE)
#Keep only states with estimate data
d <- d[d$Status %in% c("Done", "Gaurav did"),]
summary(d)
setwd("C:/Users/wguil/OneDrive/Documents/GitHub/world_happiness")
#clean R environment
rm(list = ls())
#Set working directory
setwd("C:/Users/wguil/OneDrive/Documents/GitHub/world_happiness")
knitr::opts_chunk$set(echo = TRUE)
#clean R environment
rm(list = ls())
#Set working directory
setwd("C:/Users/wguil/OneDrive/Documents/GitHub/world_happiness")
#Load R packages
d <- read.csv("./data/2019.csv", stringsAsFactors = FALSE)
#Load happiness data for 2019
d2019 <- read.csv("./data/2019.csv", stringsAsFactors = FALSE)
#Make a working copy
d <- d2019
knitr::opts_chunk$set(echo = TRUE)
#clean R environment
rm(list = ls())
#Set working directory
setwd("C:/Users/wguil/OneDrive/Documents/GitHub/world_happiness")
#Load R packages
#Load happiness data for 2019
d2019 <- read.csv("./data/2019.csv", stringsAsFactors = FALSE)
knitr::opts_chunk$set(echo = TRUE)
#clean R environment
rm(list = ls())
#Set working directory
setwd("C:/Users/wguil/OneDrive/Documents/GitHub/world_happiness/")
#Load R packages
#Load happiness data for 2019
d2019 <- read.csv("./data/2019.csv", stringsAsFactors = FALSE)
#Load happiness data for 2019
d2019 <- read.csv("./data/2019.csv", stringsAsFactors = FALSE)
#Set working directory
setwd("C:/Users/wguil/OneDrive/Documents/GitHub/world_happiness/")
#Load happiness data for 2019
d2019 <- read.csv("./data/2019.csv", stringsAsFactors = FALSE)
knitr::opts_chunk$set(echo = TRUE)
#clean R environment
rm(list = ls())
#Load R packages
#Set working directory
setwd("C:/Users/wguil/OneDrive/Documents/GitHub/world_happiness/")
#Load happiness data for 2019
d2019 <- read.csv("./data/2019.csv", stringsAsFactors = FALSE)
#Make a working copy
d <- d2019
dim(d)
str(d)
summary(cars)
plot(pressure)
colnames(d)
len(c("rank", "country", "score", "gpd_pc", "social_support", "life_expectancy", "freedom" , "generosity", "corruption"))
length(c("rank", "country", "score", "gpd_pc", "social_support", "life_expectancy", "freedom" , "generosity", "corruption"))
#Change column names to friendlier and shorter names
colnames(d) <- c("rank", "country", "score", "gpd_pc", "social_support", "life_expectancy", "freedom" , "generosity", "corruption")
#Sample of 10 observations
head(10,d)
#Sample of 10 observations
head(d,10)
#Sample of 10 observations
kable(d[1:10,], row.names = FALSE, digits =0)
#Load R packages
require(knitr)
#Load R packages
require(knitr)
knitr::opts_chunk$set(echo = TRUE)
#clean R environment
rm(list = ls())
#Load R packages
require(knitr)
#Sample of 10 observations
kable(d[1:10,], row.names = FALSE, digits =0)
d
#Set working directory
setwd("C:/Users/wguil/OneDrive/Documents/GitHub/world_happiness/")
#Load happiness data for 2019
d2019 <- read.csv("./data/2019.csv", stringsAsFactors = FALSE)
#Make a working copy
d <- d2019
#Size of the data frame
dim(d)
#Column names, type of variable and sample values
str(d)
#Change column names to friendlier and shorter names
colnames(d) <- c("rank", "country", "score", "gpd_pc", "social_support", "life_expectancy", "freedom" , "generosity", "corruption")
#Sample of 10 observations
kable(d[1:10,], row.names = FALSE, digits =0)
knitr::opts_chunk$set(echo = TRUE)
#clean R environment
rm(list = ls())
#Load R packages
require(knitr)
#Set working directory
setwd("C:/Users/wguil/OneDrive/Documents/GitHub/world_happiness/")
#Load happiness data for 2019
d2019 <- read.csv("./data/2019.csv", stringsAsFactors = FALSE)
#Make a working copy
d <- d2019
#Size of the data frame
dim(d)
#Column names, type of variable and sample values
str(d)
#Change column names to friendlier and shorter names
colnames(d) <- c("rank", "country", "score", "gpd_pc", "social_support", "life_expectancy", "freedom" , "generosity", "corruption")
#Sample of 10 observations
kable(d[1:10,], row.names = FALSE)
summary(cars)
plot(pressure)
require(amelia)
require(Amelia)
require(Amelia)
#Plot missing values
missmap(d)
#Column names, type of variable and sample values
str(d)
getwd()
#Load happiness data for 2019
d2019 <- read.csv("../data/2019.csv", stringsAsFactors = FALSE)
#Load happiness data for 2015
d2015 <- read.csv("./data/2015.csv", stringsAsFactors = FALSE)
#Load happiness data for 2015
d2015 <- read.csv("../data/2015.csv", stringsAsFactors = FALSE)
View(d2015)
d <- merge(d, d2015[,c("Country", "Region")], by.x = "country", by.y = "Country", all.x = TRUE)
View(d)
#Plot missing values
missmap(d)
#How
is.na(d$Region)
#How
sum(is.na(d$Region))
#Since it's only 7 I will address manually
d[is.na(d$Region)]
#Since it's only 7 I will address manually
d[is.na(d$Region),]
#Since it's only 7 I will address manually
d[is.na(d$Region),]$country
table(d$Region)
continents <- read.csv("../data/country-and-continent-codes-list-csv_csv.csv", stringsAsFactors = FALSE)
View(continents)
d <- merge(d, d2015[,c("Country_Name", "Continent_Name")], by.x = "country", by.y = "Country_Name", all.x = TRUE)
d <- merge(d, continents[,c("Country_Name", "Continent_Name")], by.x = "country", by.y = "Country_Name", all.x = TRUE)
View(d)
View(continents)
View(d)
View(d)
require(tidyverse)
#Set working directory
#setwd("C:/Users/wguil/OneDrive/Documents/GitHub/world_happiness/")
#Load happiness data for 2019
d2019 <- read.csv("../data/2019.csv", stringsAsFactors = FALSE)
#Make a working copy
d <- d2019
#Size of the data frame
dim(d)
#Column names, type of variable and sample values
str(d)
#Change column names to friendlier and shorter names
colnames(d) <- c("rank", "country", "score", "gpd_pc", "social_support", "life_expectancy", "freedom" , "generosity", "corruption")
#Sample of 10 observations
kable(d[1:10,], row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
#clean R environment
rm(list = ls())
#Load R packages
require(knitr)
require(Amelia)
require(tidyverse)
#Set working directory
#setwd("C:/Users/wguil/OneDrive/Documents/GitHub/world_happiness/")
#Load happiness data for 2019
d2019 <- read.csv("../data/2019.csv", stringsAsFactors = FALSE)
#Make a working copy
d <- d2019
#Size of the data frame
dim(d)
#Column names, type of variable and sample values
str(d)
#Change column names to friendlier and shorter names
colnames(d) <- c("rank", "country", "score", "gpd_pc", "social_support", "life_expectancy", "freedom" , "generosity", "corruption")
#Sample of 10 observations
kable(d[1:10,], row.names = FALSE)
#Load happiness data for 2015
d2015 <- read.csv("../data/2015.csv", stringsAsFactors = FALSE)
#Add Region to my dataset by merging by country name
d <- merge(d, d2015[,c("Country", "Region")], by.x = "country", by.y = "Country", all.x = TRUE)
#Change col name so every column is lower case for consistency
d %>% rename(region = Region)
#Let's look at region names and their count of countries
#aggregate(country ~ )
#Let's look at region names and their count of countries
aggregate(country ~ region, FUN = length)
#Let's look at region names and their count of countries
aggregate(country ~ region, data = d, FUN = length)
View(d)
#Load happiness data for 2015
d2015 <- read.csv("../data/2015.csv", stringsAsFactors = FALSE)
#Add Region to my dataset by merging by country name
d <- merge(d, d2015[,c("Country", "Region")], by.x = "country", by.y = "Country", all.x = TRUE)
#Change col name so every column is lower case for consistency
d %>% rename(region = Region)
View(d)
knitr::opts_chunk$set(echo = TRUE)
#clean R environment
rm(list = ls())
#Load R packages
require(knitr)
require(Amelia)
require(tidyverse)
#Set working directory
#setwd("C:/Users/wguil/OneDrive/Documents/GitHub/world_happiness/")
#Load happiness data for 2019
d2019 <- read.csv("../data/2019.csv", stringsAsFactors = FALSE)
#Make a working copy
d <- d2019
#Size of the data frame
dim(d)
#Column names, type of variable and sample values
str(d)
#Change column names to friendlier and shorter names
colnames(d) <- c("rank", "country", "score", "gpd_pc", "social_support", "life_expectancy", "freedom" , "generosity", "corruption")
#Sample of 10 observations
kable(d[1:10,], row.names = FALSE)
#Load happiness data for 2015
d2015 <- read.csv("../data/2015.csv", stringsAsFactors = FALSE)
#Add Region to my dataset by merging by country name
d <- merge(d, d2015[,c("Country", "Region")], by.x = "country", by.y = "Country", all.x = TRUE)
#Change col name so every column is lower case for consistency
d %>% rename(region = Region)
#Let's look at region names and their count of countries
aggregate(country ~ region, data = d, FUN = length)
View(d)
#Change col name so every column is lower case for consistency
d %>% rename(region = Region)
View(d)
#Change col name so every column is lower case for consistency
names(d)
#Change col name so every column is lower case for consistency
names(d)[names(d) == "Region"]
#Change col name so every column is lower case for consistency
names(d)[names(d) == "Region"] <- "region"
View(d)
#Let's look at region names and their count of countries
aggregate(country ~ region, data = d, FUN = length)
?table
table(d$region, useNA = "ifany")
#Let's look at region names and their count of countries
table(d$region, useNA = "ifany")
?aggregate
aggregate(country ~ region, d, FUN = length)
aggregate(country ~ region, d, FUN = length, na.rm=FALSE)
aggregate(country ~ region, d, FUN = length, na.action = 'include')
aggregate(country ~ region, d, FUN = length)
aggregate(country ~ region, d, FUN = length, na.action = na.pass)
aggregate(country ~ region, d, FUN = length, na.action = NULL)
#VOY
#This is not quite what I wanted as I wanted continents, so let's derive a continents column from this data
"Europe"
#Let's look at region names and their count of countries
table(d$region, useNA = "ifany")
d %>%
group_by(country)
d %>%
group_by(country) %>%
summarise(n = count(country))
d %>%
group_by(country) %>%
summarise(n = length(country))
d %>%
group_by(country) %>%
d %>%
group_by(country) %>%
summarise(n = n())
d %>%
group_by(country) %>%
d %>%
group_by(country) %>%
summarise(n = n())
d %>%
group_by(country) %>%
tally()
d %>%
group_by(region) %>%
tally()
d %>%
#group_by(region) %>%
count(region, sort = TRUE)
d %>%
count(region, sort = TRUE)
